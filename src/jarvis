#!/usr/bin/env python3

from mistralai import Mistral
import sys
import argparse
import os
from pygments import highlight
from pygments.lexers import get_lexer_by_name
from pygments.formatters import Terminal256Formatter
from pygments.styles import get_style_by_name
import shutil

AI_MODEL = "mistral-large-latest"
SYSTEM_PROMPT = """You are an expert software development assistant.
Focus on providing clear, concise, and practical code solutions.
- Always include necessary imports/includes
- Add brief but essential comments for complex logic
- Prefer standard libraries when possible
- Point out potential edge cases or errors
- Keep explanations short and technical
- Use code blocks with appropriate syntax
- Try to avoid existing libraries as much as possible
For simple questions, answer in one line."""

def format_code(text):
    start = text.find('```')
    if start == -1:
        return text
        
    end = text.find('```', start + 3)
    if end == -1:
        return text
    
    code_block = text[start:end + 3]
    code_lines = code_block[3:-3].strip().split('\n')
    
    language = code_lines[0].strip().lower()
    language_map = {
        'py': 'python',
        'js': 'javascript',
        'ts': 'typescript',
        'cpp': 'c++',
        'sh': 'bash'
    }
    language = language_map.get(language, language)
    
    if language and language in ['python', 'c', 'cpp', 'c++', 'javascript', 'typescript', 'bash', 'java', 'go']:
        code_lines = code_lines[1:]
    else:
        language = 'text'
        
    code = '\n'.join(code_lines)
    
    try:
        lexer = get_lexer_by_name(language)
        terminal_width = shutil.get_terminal_size().columns
        
        formatted_lines = []
        max_line_no_width = len(str(len(code_lines)))
        
        formatted_lines.append('┌' + '─' * (terminal_width - 2) + '┐')
        
        line_num = 1
        for line in code_lines:
            line_no = f"{line_num:>{max_line_no_width}} │"
            if line.strip():
                highlighted = highlight(line, lexer, Terminal256Formatter(style='monokai'))
                highlighted = highlighted.rstrip() + '\033[0m'
                padding = ' ' * (terminal_width - len(line) - len(line_no) - 4)
                formatted_lines.append(f"│ {line_no} {highlighted}{padding}│")
            else:
                formatted_lines.append(f"│ {line_no}{' ' * (terminal_width - len(line_no) - 3)}│")
            line_num += 1

        formatted_lines.append('└' + '─' * (terminal_width - 2) + '┘')

        formatted = '\n'.join(formatted_lines)

        lines_to_clear = len(code_block.split('\n'))
        print(f"\033[{lines_to_clear}A\033[J", end='')

        return formatted
    except Exception as e:
        return code

def request_ai(prompt, model=AI_MODEL):
    api_key = os.getenv('MISTRAL_API_KEY')
    if not api_key:
        raise ValueError("MISTRAL_API_KEY environment variable is required")

    client = Mistral(api_key=api_key)

    try:
        stream_response = client.chat.stream(
            model = model,
            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ]
        )
        response_text = ""
        for chunk in stream_response:
            content = chunk.data.choices[0].delta.content
            response_text += content
            print(content, end='', flush=True)
        print()

        if '```' in response_text:
            formatted = format_code(response_text)
            if formatted != response_text:
                print(formatted)

    except Exception as e:
        print(f"Error: {str(e)}")
        return None

def parse_args():
    parser = argparse.ArgumentParser(description='AI Chat Interface')
    parser.add_argument('prompt', nargs='+', help='The prompt to send to the AI')
    parser.add_argument('--model', default=AI_MODEL, help='The model to use for the AI')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    prompt = ' '.join(args.prompt)
    response = request_ai(prompt, model=args.model)
