#!/usr/bin/env python3

import ollama
import sys
import argparse

AI_MODEL = "llama3.2:1b"
SYSTEM_PROMPT = """You are a highly capable AI assistant optimized for speed. Answer the user's question as quickly as possible and write the less possible. Sometimes you can also answer with one word."""

def create_conversation(prompt, system_message=SYSTEM_PROMPT):
    return [
        {"role": "system", "content": system_message},
        {"role": "user", "content": prompt}
    ]

def request_ai(prompt, model=AI_MODEL, host="http://127.0.0.1:11434"):
    messages = create_conversation(prompt)

    try:
        for chunk in ollama.chat(
            model=model,
            messages=messages,
            stream=True
        ):
            if chunk and 'message' in chunk:
                print(chunk['message']['content'], end='', flush=True)
        print()
    except Exception as e:
        print(f"Error: {str(e)}")
        return None

def parse_args():
    parser = argparse.ArgumentParser(description='AI Chat Interface')
    parser.add_argument('prompt', nargs='+', help='The prompt to send to the AI')
    parser.add_argument('--model', default=AI_MODEL, help='Model to use')
    parser.add_argument('--host', default='http://127.0.0.1:11434', help='Host URL')
    parser.add_argument('--system', default=SYSTEM_PROMPT,
                      help='System message for the conversation')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    prompt = ' '.join(args.prompt)
    response = request_ai(prompt, model=args.model, host=args.host)
